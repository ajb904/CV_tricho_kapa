---
title: "SNP stats from Tricho assemblies"
author: "Alison Baylay"
date: "27 October 2016"
output: html_document
---

```{r setup, message=FALSE}
library(ggplot2)
library(plyr)
library(reshape2)
library(data.table)
library(sqldf)
library(zoo)
library(GenomicRanges)
library(GenomicFeatures)
library(rtracklayer)
library(VariantAnnotation)
```

Input files:

```{r}
analysis.dir <- 'assembly_reduction/cdhit/'
sample <- 'Tn004_S1_L001'
full.data.file <- paste(analysis.dir, sample, '_v_cdhit.VarScanCNSfull.tab', sep='')
vcf.file <- paste(analysis.dir, sample, '_v_cdhit.VarScanCNS.vcf', sep='')
cov.file <- paste(analysis.dir, sample, '_v_cdhit.CovSummary.tab', sep='')
gff.file <- paste(analysis.dir, sample, '_prokka/cdhit1000.gff', sep='')
fasta.file <- paste(analysis.dir, sample, '_redundans/cdhit1000.fa', sep='')

snps.vcf <- readVcf(vcf.file, genome=sample)
cov.data <- read.delim(cov.file)
gff <- import.gff(gff.file)
#Change seqlevels to match the vcf file (and the fasta file that we will use later)
seqlevels(gff) <- gsub('_', '\\|', seqlevels(gff))

#Output files
contig.data.file <- paste(analysis.dir, sample, 'contigData.csv', sep='')
CDS.data.file <- paste(analysis.dir, sample, 'CDSData.csv', sep='')

```

Plot coverage distribution, and filter VCF. We want to remove SNPs with extreme coverage depths, and also those that are only observed on one strand

```{r cov_plot}
ggplot(cov.data, aes(x=medCov)) + geom_histogram(binwidth = 10)

ssCov <- subset(cov.data, medCov < 120 & medCov >=10)
ssCov <- droplevels(ssCov)
ggplot(ssCov, aes(x=medCov)) + geom_histogram(binwidth = 5)
```

Plot per contig coverage distributions for some randomly selected contigs and check the effect of filtering positions where coverage is >1 s.d. from the mean. If this works, we can use it to filter out low coverage mutations from the main dataset.

```{r, fig.height=20}
set.seed(10)
to.check <- sample(as.character(ssCov$Chrom), 24)
ss <- subset(ssCov, Chrom %in% to.check)

sql <- paste("SELECT Chrom,Position,Reads1,Reads2 FROM file WHERE Chrom IN ( '", paste(ss$Chrom, collapse = "', '"), "')", sep = '')

pc.cov <- read.csv.sql(full.data.file, sql = sql, header = TRUE, sep='\t')

pc.cov.m <- merge(pc.cov, ss, by='Chrom')

ggplot(pc.cov.m, aes(x=Reads1+Reads2)) + geom_density() + geom_vline(aes(xintercept = meanCov),col='red') + geom_vline(aes(xintercept = meanCov-sdCov), col='blue', linetype=2) + geom_vline(aes(xintercept = meanCov+sdCov), col='blue', linetype=2) + geom_vline(aes(xintercept=medCov), col='purple', linetype=3) + facet_wrap(~ Chrom, ncol=3, scales = 'free')
```

Naive coverage based filtering:
1) Throw out all CONTIGS with median coverage > 120 or < 10
2) Deal with high coverage POSITIONS at a later stage

```{r}
snps.vcf <- snps.vcf[seqnames(snps.vcf) %in% ssCov$Chrom]
```

Strand-based filtering:
Only keep vars that are observed in at least 2 reads on both strands (GT fields RDF, RDR, ADF and ADR are all > 1)

```{r}
strand.df <- data.frame(rdf <- geno(snps.vcf)$RDF[,1],
                        rdr <- geno(snps.vcf)$RDR[,1],
                        adf <- geno(snps.vcf)$ADF[,1],
                        adr <- geno(snps.vcf)$ADR[,1])
to.keep <- subset(strand.df, rdf > 1 & rdr > 1 & adf > 1 & adr > 1)

snps.vcf <- snps.vcf[rownames(to.keep)]
```

Plot overall SNP coverage distribution

```{r}
adp <- data.frame(adp = info(snps.vcf)$ADP)
ggplot(adp, aes(x=adp)) + geom_density()

table(adp$adp > 250)
```

Make txdb object from gff file
```{r}
#If a gene doesn't have a gene name ('Name' and 'gene' fields are NA), set it to be the same as locus_tag
#Gene names are required for recognition by makeTxDb.
lt <- gff$locus_tag
gn <- gff$Name
gn[is.na(gn)] <- lt[is.na(gn)]
gff$Name <- gn
gff$gene <- gn

#Make a transcript database from gff file, to use to locate coding/non-coding variants
txdb <- makeTxDbFromGRanges(gff)
```

Table 1: Contig level data
```{r}
contig.data <- data.frame(Chrom=ssCov$Chrom, ContigLength=ssCov$ContigLength)

#Total amount of sequence that is coding/non-coding
cds.data <- gff[gff$type == 'CDS']
contig.data[,'total.coding'] <- sapply(as.character(contig.data$Chrom),
                                       function(x) sum(width(reduce(ranges(cds.data[seqnames(cds.data)==x])))))

contig.data[,'total.noncoding'] <- contig.data$ContigLength - contig.data$total.coding

contig.data[,'coding.percent'] <- contig.data$total.coding/contig.data$ContigLength * 100

#Amount of sequence encoding tRNAs
tRNA.data <- gff[gff$type == 'tRNA']
contig.data[,'total.tRNA'] <- sapply(as.character(contig.data$Chrom),
                                     function(x) sum(width(reduce(ranges(tRNA.data[seqnames(tRNA.data)==x])))))



#Locate variants: for the moment we are just interested in coding vs non-coding, so turn off promoter
#variant prediction for simplicity
loc <- locateVariants(snps.vcf, txdb, AllVariants(promoter = PromoterVariants(upstream=0, downstream=0)))

loc.summary <- ddply(as.data.frame(loc), 'seqnames', 'summarise',
                     coding.vars=sum(LOCATION=='coding'),
                     intergenic.vars=sum(LOCATION=='intergenic'))

contig.data <- merge(contig.data, loc.summary, by.x='Chrom', by.y='seqnames', all.x=TRUE)
contig.data$coding.vars[is.na(contig.data$coding.vars)] <- 0
contig.data$intergenic.vars[is.na(contig.data$intergenic.vars)] <- 0
contig.data[,'total.vars'] <- contig.data$coding.vars + contig.data$intergenic.vars

contig.data[,'total.snprate'] <- contig.data$total.vars / contig.data$ContigLength
contig.data[,'coding.snprate'] <- contig.data$coding.vars / contig.data$total.coding
contig.data[,'intergenic.snprate'] <- contig.data$intergenic.vars / contig.data$total.noncoding

#Add in coverage data to complete contig-level table
contig.data.full <- merge(contig.data, ssCov, by='Chrom')

write.csv(contig.data.full, file=contig.data.file, quote=F, row.names = F)
```

Table 2: CDS-level data

```{r}
#How many SNPs change the predicted coding sequence?
seqs <- FaFile(fasta.file, index = paste(fasta.file, '.fai', sep=''))
coding.snps <- snps.vcf[!(names(snps.vcf) %in% names(loc)[loc$LOCATION=='intergenic'])]
seqlevels(coding.snps) <- seqlevels(seqs)

pc <- predictCoding(coding.snps, txdb, seqs)
table(pc$CONSEQUENCE)

pc.df <- data.frame(Chrom=seqnames(pc), GeneID=pc$GENEID, Consequence=pc$CONSEQUENCE)

snp.by.gene <- ddply(pc.df, c('Chrom', 'GeneID', 'Consequence'), summarize, freq=length(Consequence))
snp.by.gene <- dcast(snp.by.gene, Chrom + GeneID ~ Consequence)
snp.by.gene[is.na(snp.by.gene)] <- 0

#GeneIDs should be unique?
length(unique(snp.by.gene$GeneID)) == length(unique(paste(snp.by.gene$Chrom, snp.by.gene$GeneID)))

cds.summary <- data.frame(GeneID = cds.data$gene,
                          CDSstart=start(ranges(cds.data)),
                          CDSend=end(ranges(cds.data)),
                          CDSstrand=strand(cds.data),
                          ntLength = width(ranges(cds.data)))

snp.by.gene <- merge(snp.by.gene, cds.summary, by='GeneID', sort=FALSE)

snp.by.gene[,'aaLength'] <- snp.by.gene$ntLength / 3

#Save CDS data
write.csv(snp.by.gene, file=CDS.data.file, quote=F, row.names = F)

```

KaKs ratio calculations
```{r}
#Get underlying DNA sequence. Make an edited version based on the detected polymorphisms (only use SNPs for now)
fasta <- readDNAStringSet(fasta.file)

#Extend pc.df to include ref and var alleles, and the CDS position
pc.df[,'REF'] <- pc$REF
pc.df[,'ALT'] <- unlist(pc$ALT)
pc.df[,'CDSstart'] <- start(pc$CDSLOC)
pc.df[,'CDSend'] <- end(pc$CDSLOC)


#Keep only SNPs
pc.df.snps <- subset(pc.df, nchar(REF) == 1 & nchar(ALT) == 1)


#Get CDS seqs from fasta file
cds.seqs <- getSeq(fasta, cds.data)
names(cds.seqs) <- cds.data$gene


#Make mutated CDS sequences
mutateSeq <- function(seq.name, seq.set, snp.df){
    seq.to.mutate <- seq.set[seq.name][[1]]
    positions <- as.integer(snp.df$CDSstart[snp.df$GeneID == seq.name])
    replacements <- as.character(snp.df$ALT[snp.df$GeneID == seq.name])
    
    mutated.seq <- replaceLetterAt(seq.to.mutate, positions, replacements)
    
    return(mutated.seq)
}

seqs.to.mutate <- unique(pc.df.snps$GeneID)
var.fasta <- sapply(seqs.to.mutate, function(x) mutateSeq(x, cds.seqs, pc.df.snps))
names(var.fasta) <- seqs.to.mutate

#Extract sequences of 'reference' and 'mutant' CDSs
cdsRanges <- gff[gff$type == 'CDS']

cds.seqs <- getSeq(fasta, cdsRanges)
names(cds.seqs) <- cdsRanges$gene

mut.cds.seqs <- getSeq(var.fasta, cdsRanges)
names(mut.cds.seqs) <- cdsRanges$gene

#For each pair of sequences, create an alignment object (don't need to actually run an alignment algorithm), and calculate KaKs

alignments <- lapply(1:length(cdsRanges), function(x) as.alignment(nb=2, nam=paste(names(cds.seqs)[x], c('mut', 'ref'), sep='_'), seq = c(as.character(cds.seqs[x]), as.character(mut.cds.seqs[x]))))

kaks.data <- lapply(alignments, kaks)
kaks.ratio <- sapply(kaks.data, function(x) x$ka / x$ks)
names(kaks.ratio) <- names(cds.seqs)

#Add KaKs data to summary data frame
kaks.ratio <- data.frame(KaKs.ratio = kaks.ratio)
snp.by.gene <- merge(snp.by.gene, kaks.ratio, by.x = 'gene', by.y = 'row.names')
```